{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.7 \n",
						"Current idle_timeout is None minutes.\n",
						"idle_timeout has been set to 10 minutes.\n",
						"Setting Glue version to: 5.0\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.1X\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 2\n",
						"The following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg', '--additional-python-modules': 'awswrangler==3.9.1'}\n"
					]
				}
			],
			"source": [
				"%idle_timeout 10\n",
				"%glue_version 5.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n",
				"%%configure \n",
				"{\n",
				"  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
				"  \"--datalake-formats\": \"iceberg\",\n",
				"  \"--additional-python-modules\": \"awswrangler==3.9.1, cuallee==0.15.2\"\n",
				"}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 52,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"import awswrangler as wr\n",
				"import boto3\n",
				"from pyspark.sql import SparkSession, DataFrame, types as T\n",
				"from datetime import datetime\n",
				"import uuid\n",
				"from cuallee import Check, CheckLevel"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 81,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"s3_bucket='dsc-wap-athena-20250118183836'\n",
						"catalog_name='glue_catalog'\n",
						"database_name='_wap_demo_20250118183836'\n",
						"table_name='my_iceberg_table_athena'\n",
						"full_table_name='glue_catalog._wap_demo_20250118183836.my_iceberg_table_athena'\n"
					]
				}
			],
			"source": [
				"# Athena\n",
				"athena_workgroup = \"primary\"\n",
				"\n",
				"# Glue/S3: Bucket, Database, Table\n",
				"now_string = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
				"s3_bucket = f\"wap-demo-{now_string}\" # replace with your own bucket name\n",
				"catalog_name = \"glue_catalog\"\n",
				"database_name = f\"_wap_demo_{now_string}\"\n",
				"table_name = \"my_iceberg_table_athena\"\n",
				"full_table_name = f\"{catalog_name}.{database_name}.{table_name}\"\n",
				"\n",
				"print(f\"{s3_bucket=}\")\n",
				"print(f\"{catalog_name=}\")\n",
				"print(f\"{database_name=}\")\n",
				"print(f\"{table_name=}\")\n",
				"print(f\"{full_table_name=}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 82,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"spark = SparkSession.builder \\\n",
				"    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
				"    .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
				"    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
				"    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"s3://{s3_bucket}/{catalog_name}/\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.glue.skip-name-validation\", \"true\") \\\n",
				"    .getOrCreate()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Infrastructure Setup"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 83,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"{'ResponseMetadata': {'RequestId': 'V3E31JMBEMPCDCJ4', 'HostId': 'DcDzEUSd0TEo/1hKk+LmkjIDGvulsSiuRi5DiLiyR4yFdBHJm3sEyza/ydehArD/Si7bgy17GkE=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'DcDzEUSd0TEo/1hKk+LmkjIDGvulsSiuRi5DiLiyR4yFdBHJm3sEyza/ydehArD/Si7bgy17GkE=', 'x-amz-request-id': 'V3E31JMBEMPCDCJ4', 'date': 'Sat, 18 Jan 2025 18:39:01 GMT', 'location': 'http://dsc-wap-athena-20250118183836.s3.amazonaws.com/', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'Location': 'http://dsc-wap-athena-20250118183836.s3.amazonaws.com/'}\n"
					]
				}
			],
			"source": [
				"# Create Bucket\n",
				"s3_client = boto3.client('s3')\n",
				"response = s3_client.create_bucket(Bucket=s3_bucket, CreateBucketConfiguration={'LocationConstraint': \"eu-central-1\"})\n",
				"print(response)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 42,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# Create Database\n",
				"response = wr.athena.start_query_execution(\n",
				"    sql=f\"CREATE DATABASE IF NOT EXISTS {database_name}\",\n",
				"    wait=True,\n",
				"    workgroup=athena_workgroup,\n",
				")\n",
				"print(response)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 43,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# Create Table\n",
				"response = wr.athena.start_query_execution(\n",
				"    sql=f\"\"\"\n",
				"    CREATE TABLE IF NOT EXISTS {database_name}.{table_name} (\n",
				"        id INT,\n",
				"        name STRING,\n",
				"        age INT\n",
				"    )\n",
				"    LOCATION 's3://{s3_bucket}/{database_name}/{table_name}/'\n",
				"    TBLPROPERTIES (\n",
				"      'table_type'='ICEBERG',\n",
				"      'format'='parquet',\n",
				"      'write_compression'='snappy'\n",
				"    )\"\"\",\n",
				"    wait=True,\n",
				"    workgroup=athena_workgroup,\n",
				")\n",
				"print(response)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## watch out: branches\n",
				"- Initialize 'main' branch, otherwise the table would have no branch whatsoever,\n",
				"- you need a base branch to branch off of for WAP though.\n",
				"- When not explicitly creating a branch, Iceberg will create a default one which is\n",
				"- also called 'main' when you insert data for the first time."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 44,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+----+----+-----------+-----------------------+---------------------+----------------------+\n",
						"|name|type|snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
						"+----+----+-----------+-----------------------+---------------------+----------------------+\n",
						"+----+----+-----------+-----------------------+---------------------+----------------------+\n"
					]
				}
			],
			"source": [
				"# List existing Iceberg table branches\n",
				"# This is purely informative, it could be removed or logged as INFO in a real world scenario\n",
				"spark.sql(f\"SELECT * FROM {full_table_name}.refs\").show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 45,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DataFrame[]\n"
					]
				}
			],
			"source": [
				"spark.sql(f\"ALTER TABLE {full_table_name} CREATE BRANCH IF NOT EXISTS main\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 46,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
						"|name|  type|        snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
						"+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
						"|main|BRANCH|7464855188022954752|                   NULL|                 NULL|                  NULL|\n",
						"+----+------+-------------------+-----------------------+---------------------+----------------------+\n"
					]
				}
			],
			"source": [
				"spark.sql(f\"SELECT * FROM {full_table_name}.refs\").show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# get some data and transform it"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Extract"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 47,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---+-------+---+\n",
						"| id|   name|age|\n",
						"+---+-------+---+\n",
						"|  1|  Alice| 28|\n",
						"|  2|    Bob| 34|\n",
						"|  3|Charlie| 23|\n",
						"+---+-------+---+\n"
					]
				}
			],
			"source": [
				"def read_data(spark: SparkSession) -> DataFrame:\n",
				"    # sample data\n",
				"    data = [(1, \"Alice\", 28), \n",
				"            (2, \"Bob\", 34), \n",
				"            (3, \"Charlie\", 23)] \n",
				"\n",
				"    spark_schema = T.StructType(\n",
				"        [\n",
				"            T.StructField(\"id\", T.IntegerType(), False),\n",
				"            T.StructField(\"name\", T.StringType(), False),\n",
				"            T.StructField(\"age\", T.IntegerType(), False),\n",
				"        ],\n",
				"    )\n",
				"    \n",
				"    return spark.createDataFrame(\n",
				"        data=data,\n",
				"        schema=spark_schema,\n",
				"    )\n",
				"\n",
				"\n",
				"df = read_data(spark=spark)\n",
				"df.show()\n",
				"df.printSchema()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Transform"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 48,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---+-----+---+\n",
						"| id| name|age|\n",
						"+---+-----+---+\n",
						"|  1|Alice| 28|\n",
						"|  2|  Bob| 34|\n",
						"+---+-----+---+\n"
					]
				}
			],
			"source": [
				"def transform(df: DataFrame) -> DataFrame:\n",
				"    return df.filter(df.age > 25)\n",
				"\n",
				"transformed_df = transform(df=df)\n",
				"transformed_df.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Define DQ Checks"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"def create_dq_checks() -> Check:\n",
				"    dq_checks = Check(CheckLevel.WARNING, \"CualleeChecks\")\n",
				"\n",
				"    # list of checks: \n",
				"    # https://github.com/canimus/cuallee\n",
				"    dq_checks.is_unique(\"id\")\n",
				"    dq_checks.is_complete(\"id\")\n",
				"    dq_checks.is_complete(\"name\")\n",
				"    dq_checks.is_between(\"age\", (10, 25))\n",
				"    \n",
				"    return dq_checks\n",
				"\n",
				"dq_checks = create_dq_checks()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# WAP"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## WAP Write"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 49,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DataFrame[]\n"
					]
				}
			],
			"source": [
				"# WAP: Write\n",
				"# Note: \n",
				"#  This setting is only enabled temporarily for the WAP pattern.\n",
				"#  It could be enabled permanently as far as Spark and Iceberg are concerned.\n",
				"#  When it is set, you can't use the Athena query SHOW CREATE TABLE will throw an error. \n",
				"#  An Athena SELECT query on the Iceberg table would still work even with this setting set though.\n",
				"#  But anyways, we clean it up in the finally block to enable the SHOW CREATE TABLE query again.\n",
				"spark.sql(f\"ALTER TABLE {full_table_name} SET TBLPROPERTIES ('write.wap.enabled'='true')\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Write new data into a temporary branch   "
			]
		},
		{
			"cell_type": "code",
			"execution_count": 53,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"generated branch: audit_branch_cdd0da\n"
					]
				}
			],
			"source": [
				"def generate_branch_name(prefix: str = \"branch\") -> str:\n",
				"    return f\"{prefix}_{uuid.uuid4().hex[:6]}\"\n",
				"\n",
				"audit_branch_name = generate_branch_name(prefix=\"audit_branch\")\n",
				"print(f\"generated branch: {audit_branch_name}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 55,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
						"|name|  type|        snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
						"+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
						"|main|BRANCH|7464855188022954752|                   NULL|                 NULL|                  NULL|\n",
						"+----+------+-------------------+-----------------------+---------------------+----------------------+\n"
					]
				}
			],
			"source": [
				"spark.sql(f\"SELECT * FROM {full_table_name}.refs\").show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 57,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+-------------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
						"|               name|  type|        snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
						"+-------------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
						"|audit_branch_cdd0da|BRANCH|7464855188022954752|                   NULL|                 NULL|                  NULL|\n",
						"|               main|BRANCH|7464855188022954752|                   NULL|                 NULL|                  NULL|\n",
						"+-------------------+------+-------------------+-----------------------+---------------------+----------------------+\n"
					]
				}
			],
			"source": [
				"spark.sql(f\"ALTER TABLE {full_table_name} DROP BRANCH IF EXISTS {audit_branch_name}\")\n",
				"spark.sql(f\"ALTER TABLE {full_table_name} CREATE BRANCH {audit_branch_name}\")\n",
				"spark.sql(f\"SELECT * FROM {full_table_name}.refs\").show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"### writing data into audit_branch"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 58,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"(transformed_df.write\n",
				"    .format(\"iceberg\")\n",
				"    .mode(\"append\")\n",
				"    .option(\"branch\", audit_branch_name)\n",
				"    .save(path=full_table_name))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### only audit_branch has the new data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 59,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---+----+---+\n",
						"| id|name|age|\n",
						"+---+----+---+\n",
						"+---+----+---+\n"
					]
				}
			],
			"source": [
				"spark.read \\\n",
				"    .format(\"iceberg\") \\\n",
				"    .load(path=full_table_name).show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 60,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---+-------+---+\n",
						"| id|   name|age|\n",
						"+---+-------+---+\n",
						"|  1|  Alice| 28|\n",
						"|  2|    Bob| 34|\n",
						"|  3|Charlie| 23|\n",
						"+---+-------+---+\n"
					]
				}
			],
			"source": [
				"spark.read \\\n",
				"    .format(\"iceberg\") \\\n",
				"    .option(\"branch\", audit_branch_name) \\\n",
				"    .load(path=full_table_name).show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# WAP: Audit\n",
				"\n",
				"Audit temporary branch\n",
				"\n",
				"**Note:**\n",
				"- This is for demonstration, in a real world scenario you would want to do a more complex audit.\n",
				"    - You could for example refactor this function and inject a test suite to run on the branch_df.\n",
				"    - You could also differentiate between the severity of failures, i.e. \"warning\" or \"failing\" checks.\n",
				"\n",
				"**Note:**\n",
				"- The Audit is very likely the most interesting part from a business value and analytics perspective.\n",
				"- Getting the business rules right is where you should focus your attention during development."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 61,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"branch_df = spark.read \\\n",
				"    .format(\"iceberg\") \\\n",
				"    .option(\"branch\", audit_branch_name) \\\n",
				"    .load(path=full_table_name)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 68,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"data quality checks passed\n"
					]
				}
			],
			"source": [
				"def create_dq_checks() -> Check:\n",
				"    dq_checks = Check(CheckLevel.WARNING, \"CualleeChecks\")\n",
				"\n",
				"    # list of checks: \n",
				"    # https://github.com/canimus/cuallee\n",
				"    dq_checks.is_unique(\"id\")\n",
				"    dq_checks.is_complete(\"id\")\n",
				"    dq_checks.is_complete(\"name\")\n",
				"    dq_checks.is_between(\"age\", (10, 25))\n",
				"    \n",
				"    return dq_checks\n",
				"\n",
				"dq_checks = create_dq_checks()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"df_results = audit_df(df=branch_df,\n",
				"                      dq_checks=dq_checks)\n",
				"audit_passed = (df_results.count() < 1)\n",
				"\n",
				"if audit_passed:\n",
				"    print(\"data quality checks passed\")\n",
				"else:\n",
				"    print(\"data quality checks failed\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# WAP: Publish\n",
				"- On the happy path all checks passed\n",
				"- Publish changes from temporary branch to main branch"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 69,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DataFrame[branch_updated: string, previous_ref: bigint, updated_ref: bigint]\n"
					]
				}
			],
			"source": [
				"# Fast-forward merge: [audit_branch] -> [main] \n",
				"spark.sql(f\"CALL {catalog_name}.system.fast_forward('{full_table_name}', 'main', '{audit_branch_name}')\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 70,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---+-------+---+\n",
						"| id|   name|age|\n",
						"+---+-------+---+\n",
						"|  1|  Alice| 28|\n",
						"|  2|    Bob| 34|\n",
						"|  3|Charlie| 23|\n",
						"+---+-------+---+\n"
					]
				}
			],
			"source": [
				"spark.read \\\n",
				"    .format(\"iceberg\") \\\n",
				"    .load(path=full_table_name).show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 71,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---+-------+---+\n",
						"| id|   name|age|\n",
						"+---+-------+---+\n",
						"|  1|  Alice| 28|\n",
						"|  2|    Bob| 34|\n",
						"|  3|Charlie| 23|\n",
						"+---+-------+---+\n"
					]
				}
			],
			"source": [
				"spark.read \\\n",
				"    .format(\"iceberg\") \\\n",
				"    .option(\"branch\", audit_branch_name) \\\n",
				"    .load(path=full_table_name).show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 72,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DataFrame[]\n"
					]
				}
			],
			"source": [
				"# The Audit is done. Thus the audit branch has served its purpose and can be deleted.\n",
				"# Note:\n",
				"#  This is explicitly NOT part of the finally block, because you might want to analyze\n",
				"#  the data in the audit branch in case of data quality check failures and only delete it afterwards.\n",
				"#  This is especially true in case computing the results is expensive.\n",
				"#  You could also argue against this decision though, for example in case you need\n",
				"#  to avoid manual interventions in prod altogether, or in case you know you won't analyze results anyway.\n",
				"#  Being able to look at the faulty results is generally something you want though.\n",
				"spark.sql(f\"ALTER TABLE {full_table_name} DROP BRANCH {audit_branch_name}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 73,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---+-------+---+\n",
						"| id|   name|age|\n",
						"+---+-------+---+\n",
						"|  1|  Alice| 28|\n",
						"|  2|    Bob| 34|\n",
						"|  3|Charlie| 23|\n",
						"+---+-------+---+\n"
					]
				}
			],
			"source": [
				"spark.read \\\n",
				"    .format(\"iceberg\") \\\n",
				"    .load(path=full_table_name).show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 74,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Py4JJavaError: An error occurred while calling o424.load.\n",
						": org.apache.iceberg.exceptions.ValidationException: Cannot use branch (does not exist): audit_branch_cdd0da\n",
						"\tat org.apache.iceberg.exceptions.ValidationException.check(ValidationException.java:49)\n",
						"\tat org.apache.iceberg.spark.source.SparkTable.<init>(SparkTable.java:135)\n",
						"\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:902)\n",
						"\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:172)\n",
						"\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:355)\n",
						"\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n",
						"\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.$anonfun$lookupAndLoadDataSource$1(DataSourceV2Utils.scala:168)\n",
						"\tat scala.Option.flatMap(Option.scala:271)\n",
						"\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.lookupAndLoadDataSource(DataSourceV2Utils.scala:166)\n",
						"\tat org.apache.spark.sql.DataFrameReader.loadV2Source(DataFrameReader.scala:241)\n",
						"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
						"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n",
						"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
						"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
						"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
						"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
						"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
						"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
						"\tat py4j.Gateway.invoke(Gateway.java:282)\n",
						"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
						"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
						"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
						"\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
						"\n"
					]
				}
			],
			"source": [
				"# The temporary audit branch doesn't exist anymore so we expect this to fail\n",
				"spark.read \\\n",
				"    .format(\"iceberg\") \\\n",
				"    .option(\"branch\", audit_branch_name) \\\n",
				"    .load(path=full_table_name).show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Und der Fehlerfall?"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 65,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Audit failed. Not publishing changes.\n"
					]
				}
			],
			"source": [
				"# WAP: Don't publish\n",
				"# Because when a check failed we know there's an issue with the data\n",
				"# Note:\n",
				"#  In a real world scenario you would want to do a more complex Data Quality Check failure handling,\n",
				"#  i.e. construct an audit report, send an email to the team, to the consumers, log the error, etc.\n",
				"print(\"Audit failed. Not publishing changes.\")\n",
				"df_results.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Cleanup"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 75,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DataFrame[]\n"
					]
				}
			],
			"source": [
				"spark.sql(f\"ALTER TABLE {full_table_name} UNSET TBLPROPERTIES ('write.wap.enabled')\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 77,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"{'QueryExecutionId': 'eab34f9a-308f-4986-a5b2-62fa7f94d46e', 'Query': 'DROP DATABASE IF EXISTS _wap_demo_2025_01_18_18_23_23 CASCADE', 'StatementType': 'DDL', 'ResultConfiguration': {'OutputLocation': 's3://dev-athena-results/results/eab34f9a-308f-4986-a5b2-62fa7f94d46e.txt', 'EncryptionConfiguration': {'EncryptionOption': 'SSE_KMS', 'KmsKey': 'alias/key-bucket-default'}}, 'ResultReuseConfiguration': {'ResultReuseByAgeConfiguration': {'Enabled': False}}, 'QueryExecutionContext': {}, 'Status': {'State': 'SUCCEEDED', 'SubmissionDateTime': datetime.datetime(2025, 1, 18, 18, 34, 16, 262000, tzinfo=tzlocal()), 'CompletionDateTime': datetime.datetime(2025, 1, 18, 18, 34, 17, 755000, tzinfo=tzlocal())}, 'Statistics': {'EngineExecutionTimeInMillis': 1415, 'DataScannedInBytes': 0, 'TotalExecutionTimeInMillis': 1493, 'QueryQueueTimeInMillis': 36, 'ServicePreProcessingTimeInMillis': 17, 'ServiceProcessingTimeInMillis': 25, 'ResultReuseInformation': {'ReusedPreviousResult': False}}, 'WorkGroup': 'primary', 'EngineVersion': {'SelectedEngineVersion': 'AUTO', 'EffectiveEngineVersion': 'Athena engine version 3'}, 'SubstatementType': 'DROP_DATABASE'}\n"
					]
				}
			],
			"source": [
				"wr.athena.start_query_execution(\n",
				"    sql=f\"DROP DATABASE IF EXISTS {database_name} CASCADE\",\n",
				"    wait=True,\n",
				"    workgroup=athena_workgroup,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"s3_client.delete_bucket(Bucket=s3_bucket)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}

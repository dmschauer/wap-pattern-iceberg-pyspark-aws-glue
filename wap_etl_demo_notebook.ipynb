{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 10\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\",\n  \"--additional-python-modules\": \"awswrangler==3.9.1\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 10 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg', '--additional-python-modules': 'awswrangler==3.9.1'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import awswrangler as wr\nimport boto3\nfrom pyspark.sql import DataFrame, SparkSession\nfrom datetime import datetime\nimport uuid",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Athena\nathena_workgroup = \"primary\"\n\n# Glue/S3: Bucket, Database, Table\nnow_string = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ns3_bucket = f\"wap-demo-{now_string}\" # replace with your own bucket name\ncatalog_name = \"glue_catalog\"\ndatabase_name = f\"_wap_demo_{now_string}\"\ntable_name = \"my_iceberg_table_athena\"\nfull_table_name = f\"{catalog_name}.{database_name}.{table_name}\"\n\nprint(f\"{s3_bucket=}\")\nprint(f\"{catalog_name=}\")\nprint(f\"{database_name=}\")\nprint(f\"{table_name=}\")\nprint(f\"{full_table_name=}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 81,
			"outputs": [
				{
					"name": "stdout",
					"text": "s3_bucket='dsc-wap-athena-20250118183836'\ncatalog_name='glue_catalog'\ndatabase_name='_wap_demo_20250118183836'\ntable_name='my_iceberg_table_athena'\nfull_table_name='glue_catalog._wap_demo_20250118183836.my_iceberg_table_athena'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark = SparkSession.builder \\\n    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n    .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"s3://{s3_bucket}/{catalog_name}/\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.glue.skip-name-validation\", \"true\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 82,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Infrastructure Setup",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Create Bucket\ns3_client = boto3.client('s3')\nresponse = s3_client.create_bucket(Bucket=s3_bucket, CreateBucketConfiguration={'LocationConstraint': \"eu-central-1\"})\nprint(response)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 83,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'ResponseMetadata': {'RequestId': 'V3E31JMBEMPCDCJ4', 'HostId': 'DcDzEUSd0TEo/1hKk+LmkjIDGvulsSiuRi5DiLiyR4yFdBHJm3sEyza/ydehArD/Si7bgy17GkE=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'DcDzEUSd0TEo/1hKk+LmkjIDGvulsSiuRi5DiLiyR4yFdBHJm3sEyza/ydehArD/Si7bgy17GkE=', 'x-amz-request-id': 'V3E31JMBEMPCDCJ4', 'date': 'Sat, 18 Jan 2025 18:39:01 GMT', 'location': 'http://dsc-wap-athena-20250118183836.s3.amazonaws.com/', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'Location': 'http://dsc-wap-athena-20250118183836.s3.amazonaws.com/'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create Database\nresponse = wr.athena.start_query_execution(\n    sql=f\"CREATE DATABASE IF NOT EXISTS {database_name}\",\n    wait=True,\n    workgroup=athena_workgroup,\n)\nprint(response)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create Table\nresponse = wr.athena.start_query_execution(\n    sql=f\"\"\"\n    CREATE TABLE IF NOT EXISTS {database_name}.{table_name} (\n        id INT,\n        name STRING,\n        age INT\n    )\n    LOCATION 's3://{s3_bucket}/{database_name}/{table_name}/'\n    TBLPROPERTIES (\n      'table_type'='ICEBERG',\n      'format'='parquet',\n      'write_compression'='snappy'\n    )\"\"\",\n    wait=True,\n    workgroup=athena_workgroup,\n)\nprint(response)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## watch out: branches\n- Initialize 'main' branch, otherwise the table would have no branch whatsoever,\n- you need a base branch to branch off of for WAP though.\n- When not explicitly creating a branch, Iceberg will create a default one which is\n- also called 'main' when you insert data for the first time.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# List existing Iceberg table branches\n# This is purely informative, it could be removed or logged as INFO in a real world scenario\nspark.sql(f\"SELECT * FROM {full_table_name}.refs\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+----+-----------+-----------------------+---------------------+----------------------+\n|name|type|snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n+----+----+-----------+-----------------------+---------------------+----------------------+\n+----+----+-----------+-----------------------+---------------------+----------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"ALTER TABLE {full_table_name} CREATE BRANCH IF NOT EXISTS main\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"SELECT * FROM {full_table_name}.refs\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+------+-------------------+-----------------------+---------------------+----------------------+\n|name|  type|        snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n+----+------+-------------------+-----------------------+---------------------+----------------------+\n|main|BRANCH|7464855188022954752|                   NULL|                 NULL|                  NULL|\n+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# get some data and transform it",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Extract",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def read_data(spark: SparkSession) -> DataFrame:\n    # sample data\n    return spark.createDataFrame(data=[\n        (1, \"Alice\", 28),\n        (2, \"Bob\", 34),\n        (3, \"Charlie\", 23)\n    ], schema=[\"id\", \"name\", \"age\"])\n\n\ndf = read_data(spark=spark)\ndf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  1|  Alice| 28|\n|  2|    Bob| 34|\n|  3|Charlie| 23|\n+---+-------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Transform",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def transform(df: DataFrame) -> DataFrame:\n    return df.filter(df.age > 25)\n\ntransformed_df = transform(df=df)\ntransformed_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  1|Alice| 28|\n|  2|  Bob| 34|\n+---+-----+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# WAP",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## WAP Write",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# WAP: Write\n# Note: \n#  This setting is only enabled temporarily for the WAP pattern.\n#  It could be enabled permanently as far as Spark and Iceberg are concerned.\n#  When it is set, you can't use the Athena query SHOW CREATE TABLE will throw an error. \n#  An Athena SELECT query on the Iceberg table would still work even with this setting set though.\n#  But anyways, we clean it up in the finally block to enable the SHOW CREATE TABLE query again.\nspark.sql(f\"ALTER TABLE {full_table_name} SET TBLPROPERTIES ('write.wap.enabled'='true')\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Write new data into a temporary branch   ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def generate_branch_name(prefix: str = \"branch\") -> str:\n    return f\"{prefix}_{uuid.uuid4().hex[:6]}\"\n\naudit_branch_name = generate_branch_name(prefix=\"audit_branch\")\nprint(f\"generated branch: {audit_branch_name}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 53,
			"outputs": [
				{
					"name": "stdout",
					"text": "generated branch: audit_branch_cdd0da\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"SELECT * FROM {full_table_name}.refs\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 55,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+------+-------------------+-----------------------+---------------------+----------------------+\n|name|  type|        snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n+----+------+-------------------+-----------------------+---------------------+----------------------+\n|main|BRANCH|7464855188022954752|                   NULL|                 NULL|                  NULL|\n+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"ALTER TABLE {full_table_name} DROP BRANCH IF EXISTS {audit_branch_name}\")\nspark.sql(f\"ALTER TABLE {full_table_name} CREATE BRANCH {audit_branch_name}\")\nspark.sql(f\"SELECT * FROM {full_table_name}.refs\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 57,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+------+-------------------+-----------------------+---------------------+----------------------+\n|               name|  type|        snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n+-------------------+------+-------------------+-----------------------+---------------------+----------------------+\n|audit_branch_cdd0da|BRANCH|7464855188022954752|                   NULL|                 NULL|                  NULL|\n|               main|BRANCH|7464855188022954752|                   NULL|                 NULL|                  NULL|\n+-------------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### writing data into audit_branch",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "(df.write\n    .format(\"iceberg\")\n    .mode(\"append\")\n    .option(\"branch\", audit_branch_name)\n    .save(path=full_table_name))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 58,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### only audit_branch has the new data",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark.read \\\n    .format(\"iceberg\") \\\n    .load(path=full_table_name).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+----+---+\n| id|name|age|\n+---+----+---+\n+---+----+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.read \\\n    .format(\"iceberg\") \\\n    .option(\"branch\", audit_branch_name) \\\n    .load(path=full_table_name).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 60,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  1|  Alice| 28|\n|  2|    Bob| 34|\n|  3|Charlie| 23|\n+---+-------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# WAP: Audit\n\nAudit temporary branch\n\n**Note:**\n- This is for demonstration, in a real world scenario you would want to do a more complex audit.\n    - You could for example refactor this function and inject a test suite to run on the branch_df.\n    - You could also differentiate between the severity of failures, i.e. \"warning\" or \"failing\" checks.\n\n**Note:**\n- The Audit is very likely the most interesting part from a business value and analytics perspective.\n- Getting the business rules right is where you should focus your attention during development.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "branch_df = spark.read \\\n    .format(\"iceberg\") \\\n    .option(\"branch\", audit_branch_name) \\\n    .load(path=full_table_name)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "audit_case_1 = branch_df.count() == df.count()\naudit_case_2 = branch_df.count() > 0\naudit_passed = audit_case_1 and audit_case_2\n\nif audit_passed:\n    print(\"data quality checks passed\")\nelse:\n    print(\"data quality checks failed\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 68,
			"outputs": [
				{
					"name": "stdout",
					"text": "data quality checks passed\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# WAP: Publish\n- On the happy path all checks passed\n- Publish changes from temporary branch to main branch",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Fast-forward merge: [audit_branch] -> [main] \nspark.sql(f\"CALL {catalog_name}.system.fast_forward('{full_table_name}', 'main', '{audit_branch_name}')\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 69,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[branch_updated: string, previous_ref: bigint, updated_ref: bigint]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.read \\\n    .format(\"iceberg\") \\\n    .load(path=full_table_name).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 70,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  1|  Alice| 28|\n|  2|    Bob| 34|\n|  3|Charlie| 23|\n+---+-------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.read \\\n    .format(\"iceberg\") \\\n    .option(\"branch\", audit_branch_name) \\\n    .load(path=full_table_name).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 71,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  1|  Alice| 28|\n|  2|    Bob| 34|\n|  3|Charlie| 23|\n+---+-------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# The Audit is done. Thus the audit branch has served its purpose and can be deleted.\n# Note:\n#  This is explicitly NOT part of the finally block, because you might want to analyze\n#  the data in the audit branch in case of data quality check failures and only delete it afterwards.\n#  This is especially true in case computing the results is expensive.\n#  You could also argue against this decision though, for example in case you need\n#  to avoid manual interventions in prod altogether, or in case you know you won't analyze results anyway.\n#  Being able to look at the faulty results is generally something you want though.\nspark.sql(f\"ALTER TABLE {full_table_name} DROP BRANCH {audit_branch_name}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 72,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.read \\\n    .format(\"iceberg\") \\\n    .load(path=full_table_name).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 73,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  1|  Alice| 28|\n|  2|    Bob| 34|\n|  3|Charlie| 23|\n+---+-------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# The temporary audit branch doesn't exist anymore so we expect this to fail\nspark.read \\\n    .format(\"iceberg\") \\\n    .option(\"branch\", audit_branch_name) \\\n    .load(path=full_table_name).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 74,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o424.load.\n: org.apache.iceberg.exceptions.ValidationException: Cannot use branch (does not exist): audit_branch_cdd0da\n\tat org.apache.iceberg.exceptions.ValidationException.check(ValidationException.java:49)\n\tat org.apache.iceberg.spark.source.SparkTable.<init>(SparkTable.java:135)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:902)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:172)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:355)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.$anonfun$lookupAndLoadDataSource$1(DataSourceV2Utils.scala:168)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.lookupAndLoadDataSource(DataSourceV2Utils.scala:166)\n\tat org.apache.spark.sql.DataFrameReader.loadV2Source(DataFrameReader.scala:241)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Und der Fehlerfall?",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# WAP: Don't publish\n# Because when a check failed we know there's an issue with the data\n# Note:\n#  In a real world scenario you would want to do a more complex Data Quality Check failure handling,\n#  i.e. construct an audit report, send an email to the team, to the consumers, log the error, etc.\nprint(\"Audit failed. Not publishing changes.\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 65,
			"outputs": [
				{
					"name": "stdout",
					"text": "Audit failed. Not publishing changes.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Cleanup",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"ALTER TABLE {full_table_name} UNSET TBLPROPERTIES ('write.wap.enabled')\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 75,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "wr.athena.start_query_execution(\n    sql=f\"DROP DATABASE IF EXISTS {database_name} CASCADE\",\n    wait=True,\n    workgroup=athena_workgroup,\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 77,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'QueryExecutionId': 'eab34f9a-308f-4986-a5b2-62fa7f94d46e', 'Query': 'DROP DATABASE IF EXISTS _wap_demo_2025_01_18_18_23_23 CASCADE', 'StatementType': 'DDL', 'ResultConfiguration': {'OutputLocation': 's3://dev-athena-results/results/eab34f9a-308f-4986-a5b2-62fa7f94d46e.txt', 'EncryptionConfiguration': {'EncryptionOption': 'SSE_KMS', 'KmsKey': 'alias/key-bucket-default'}}, 'ResultReuseConfiguration': {'ResultReuseByAgeConfiguration': {'Enabled': False}}, 'QueryExecutionContext': {}, 'Status': {'State': 'SUCCEEDED', 'SubmissionDateTime': datetime.datetime(2025, 1, 18, 18, 34, 16, 262000, tzinfo=tzlocal()), 'CompletionDateTime': datetime.datetime(2025, 1, 18, 18, 34, 17, 755000, tzinfo=tzlocal())}, 'Statistics': {'EngineExecutionTimeInMillis': 1415, 'DataScannedInBytes': 0, 'TotalExecutionTimeInMillis': 1493, 'QueryQueueTimeInMillis': 36, 'ServicePreProcessingTimeInMillis': 17, 'ServiceProcessingTimeInMillis': 25, 'ResultReuseInformation': {'ReusedPreviousResult': False}}, 'WorkGroup': 'primary', 'EngineVersion': {'SelectedEngineVersion': 'AUTO', 'EffectiveEngineVersion': 'Athena engine version 3'}, 'SubstatementType': 'DROP_DATABASE'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3_client.delete_bucket(Bucket=s3_bucket)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}